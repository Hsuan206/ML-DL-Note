###### tags: `NLP learning`

# 前處理

[TOC]

## Load dataset


* 以 [SMS spam collection dataset](https://www.kaggle.com/ishansoni/sms-spam-collection-dataset) 為範例

**Example**
```python=
import pandas as pd

# 文件路徑
corpus_root = 'text/spam.csv'  # 自行定義

'''
讀取檔案
'''
def readData_rawSMS(filepath):
    data_rawSMS   = pd.read_csv(filepath,usecols=[0,1],encoding='latin-1')
    data_rawSMS.columns=['label','content']
    return data_rawSMS

data_rawSMS = readData_rawSMS(corpus_root)

# 增設一個欄位用0或1識別是否為垃圾信，方便後續模型分類
data_rawSMS['spam'] = data_rawSMS['label'].map(lambda x : 0 if (x=='ham') else 1)
data_rawSMS.head()
```
output：
![](https://i.imgur.com/ofmdVAl.png)

## Tokenization

* 取字、句或是段落等等，依照需求處理拆分

**Example**
```python=
from nltk.tokenize import word_tokenize, TreebankWordTokenizer

'''
取一個一個的字
'''
def tokenize(row):

#     tokenizer = TreebankWordTokenizer()
#     tokens = tokenizer.tokenize(row['content'])
    tokens = word_tokenize(row['content'])
    tokens = " ".join(tokens)
    return tokens

data_rawSMS['content'] = data_rawSMS.apply(tokenize, axis=1)
data_rawSMS.head()
```
output：
![](https://i.imgur.com/YlaFsEm.png)

> ### 小補充
> [color=#398bc6] **有文章說 TreebankWordTokenizer 接近我們想要的英語的 tokenize 方法，但是與 word_tokenize 沒什麼太大的差異**

>### 參考資料
>
> [NLP Text Preprocessing](https://towardsdatascience.com/all-you-need-to-know-about-nlp-text-preprocessing-4e55e349bdb3)
>

## Remove punctuation & find alphabets (含轉小寫)

* 去除所有的標點符號
* re 正規表達式，檢查一字串是否匹配某種模式 (pattern)：
可以使用 re.sub(pattern, input_str) 去除不需要分析的數字，或是直接使用 re.findall(pattern, input_str) 找到需要的字
* 轉換小寫

**Example**
```python=
import string
import re

'''
刪除標點符號以及只找英文字母
'''
def remove_punctuation_and_find_alphabet(row):
    
    # 尋找不是標點符號的字
    no_punctuation = [char for char in row['content'] if char not in string.punctuation]
    no_punctuation = "".join(no_punctuation)
    # 尋找英文字母
    no_punctuation_alpha = re.findall('[A-Za-z]+', no_punctuation)
    # 轉小寫
    no_punctuation_alpha = [word.lower() for word in no_punctuation_alpha]
    
    return no_punctuation_alpha
    
data_rawSMS['content'] = data_rawSMS.apply(remove_punctuation_and_find_alphabet, axis=1)
data_rawSMS.head()
```
output：
![](https://i.imgur.com/gc2FvwT.png)


:sparkles: **index[4] 的 n't 變成了 nt**
:sparkles: **原本 index[2] 的數字 2 也不見哩**
:sparkles: **都變成小寫**


## Remove not English words (也可以不做)


**Example**
```python=
'''
載入英文單字檔案
'''
def load_words():
    with open('text/words.txt') as word_file:
        valid_words = set(word_file.read().split())
        valid_words = [word.lower() for word in valid_words]
    return valid_words
    
english_words = load_words()

'''
移除不是英文單字的字，因為有時候會出現 XXXXX 之類的
'''
def remove_not_english_words(row):
    eng_words = [w for w in row['content'] if w in english_words]
    return eng_words

data_rawSMS['content'] = data_rawSMS.apply(remove_not_english_words, axis=1)
data_rawSMS.head()
```
output：
![](https://i.imgur.com/gZD1n9E.png)
:sparkles: **index[1] 的 wif 被刪除了**

>### 參考資料
>
> [English 來源](https://github.com/dwyl/english-words)
>

## Lemmatization

* 詞型還原：將字詞形式減少到共同的基礎型態，比較能獲得正確的單字形式，不會過度精簡

**Example**
```python=
from nltk.stem import WordNetLemmatizer 
'''
Lemmatization，將詞性還原
'''
def lemma(row):
    WNLemma = WordNetLemmatizer() 
    return [WNLemma.lemmatize(w) for w in row['content']]
    
data_rawSMS['content'] = data_rawSMS.apply(lemma, axis=1)
data_rawSMS.head()
```
output：
![](https://i.imgur.com/jXG10ar.png)

:sparkles: **index[4] goes 變成 go**


## Remove stop words

* 刪除沒有意義的詞

**Example**
```python=
from nltk.corpus import stopwords

'''
去除 stop word
'''
def stopword(row):
    words = [word for word in row['content']]
    stoplist = set(stopwords.words("english"))
    
    # 去除長度 < 3 的字以及不重要的字
    word_filter = [w for w in words if (len(w)>2) & (w not in stoplist)]
    return word_filter
    
data_rawSMS['content'] = data_rawSMS.apply(stopword, axis=1)
data_rawSMS.head()
```
output：
![](https://i.imgur.com/ut1u9Z4.png)

:sparkles: **一些 in、on 還有 u、c 等等的字都被刪除**

## Remove not String data type
* 因為有些欄位值可能為空值(nan) 所以需要把非字串的資料列刪除

**Example**
```python=
data_SMS = data_SMS[[type(x)==str for x in data_SMS['content']]]
```


> ### 小補充
> [color=#398bc6] 
> 
    可以偷看一下 spam 和 ham 分別常出現的字，有看過也有人直接一開始就用文字雲弄出來

**Example**
```python=
import nltk
'''
讀取內容
'''
spam_word = data_rawSMS[data_rawSMS['spam']==1]['content']
ham_word = data_rawSMS[data_rawSMS['spam']==0]['content']

# spam word
list_spam_words = []
for sublist in spam_word:
    for item in sublist:
        list_spam_words.append(item)
        
# ham word       
list_ham_words = []
for sublist in ham_word:
    for item in sublist:
        list_ham_words.append(item)

# 計算單字出現次數的 dictionary 
spam_word_freq = nltk.FreqDist(list_spam_words)
print(spam_word_freq)
 
ham_word_freq = nltk.FreqDist(list_ham_words)
print(ham_word_freq)
```
output：
**spam**
![](https://i.imgur.com/fzE4ZBx.png)
**ham**
![](https://i.imgur.com/HFVibpz.png)

    
## TF-IDF


**Example1**
```python=
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = data_rawSMS['content']

'''
直接轉換成 TD-IDF 的向量
'''
tfidf = TfidfVectorizer()
data_tfidf = tfidf.fit_transform(corpus)
print(data_tfidf)
```

**Example2**
```python=
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

corpus = data_rawSMS['content']

'''
先計算詞頻的向量，再轉換成 TD-IDF
'''
vectorizer = CountVectorizer()
transformer = TfidfTransformer()
data_tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus)) 
print (data_tfidf)
```

output：
![](https://i.imgur.com/pwQXYXi.png)

**Example3**
```python=
from sklearn.feature_extraction.text import TfidfVectorizer
corpus = data_rawSMS['content']

'''
轉換成 array 放到一個欄位
'''
tfidf = TfidfVectorizer()
data_rawSMS['textsVect'] = list(tfidf.fit_transform(corpus).toarray())

data_rawSMS.head()
```

output：
![](https://i.imgur.com/0UoFblr.png)
